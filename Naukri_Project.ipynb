{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install selenium using command \" pip install selenium \"\n",
        "\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.common.by import By\n",
        "import time\n",
        "import csv\n",
        "\n",
        "driver = webdriver.Chrome()\n",
        "wait = WebDriverWait(driver, 20)"
      ],
      "metadata": {
        "id": "xjipt_d3zIoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Update the URL of Naukri Page! ( Make Sure that the page link which you're putting must be a job listing page and it must have Next page buttons. )\n",
        "driver.get(\"https://www.naukri.com/jobs-in-chennai-1\")\n",
        "\n",
        "count = 100  # Update the Number of Vacancy count you want to scrape.\n",
        "\n",
        "index, new_index, i = '0', 1, 0  # index variable of the elements from which data will be Scraped\n",
        "# Xpaths of the various element from which data will be scraped.\n",
        "heading_xpath = '(//*[@class=\"srp-jobtuple-wrapper\"])['+index+']/div/div/a'\n",
        "link_xpath = '(//*[@class=\"srp-jobtuple-wrapper\"])['+index+']/div/div/a'\n",
        "subheading_xpath = '(//*[@class=\"srp-jobtuple-wrapper\"])['+index+']/div/div[2]//a'\n",
        "experience_xpath = '(//*[@class=\"srp-jobtuple-wrapper\"])['+index+']/div/div[3]/div/span[1]/span/span'\n",
        "salary_xpath = '(//*[@class=\"srp-jobtuple-wrapper\"])['+index+']/div/div[3]/div/span[2]/span/span'\n",
        "\n",
        "csv_file = open('Naukri_scrape.csv', 'a', encoding=\"utf-8\", newline='')\n",
        "csv_writer = csv.writer(csv_file)\n",
        "# Writing the Heading of CSV file.\n",
        "csv_writer.writerow(['Heading', 'Sub Heading', 'Vacancy Link', 'Experience Needed', 'Salary'])\n",
        "\n",
        "while i < count:\n",
        "\n",
        "    for j in range(20):\n",
        "        # Here we're replacing the Old index count of Xpath with New Index count.\n",
        "        temp_index = str(new_index).zfill(2)  # Zfill(2) is used to put zeros to the left of any digit till 2 decimal places.\n",
        "        heading_xpath = heading_xpath.replace(index, temp_index)\n",
        "        link_xpath = link_xpath.replace(index, temp_index)\n",
        "        subheading_xpath = subheading_xpath.replace(index, temp_index)\n",
        "        experience_xpath = experience_xpath.replace(index, temp_index)\n",
        "        salary_xpath = salary_xpath.replace(index, temp_index)\n",
        "        index = str(new_index).zfill(2)\n",
        "        try:\n",
        "            # Capturing the Heading from webpage and storing that into Heading variable.\n",
        "            heading = wait.until(EC.presence_of_element_located((By.XPATH, heading_xpath))).text\n",
        "            print(heading)\n",
        "        except:\n",
        "            heading = \"NULL\"\n",
        "        try:\n",
        "            link = wait.until(EC.presence_of_element_located((By.XPATH, link_xpath))).get_attribute('href')\n",
        "            print(link)\n",
        "        except:\n",
        "            link = \"NULL\"\n",
        "        try:\n",
        "            subheading = wait.until(EC.presence_of_element_located((By.XPATH, subheading_xpath))).text\n",
        "            print(subheading)\n",
        "        except:\n",
        "            subheading = \"NULL\"\n",
        "        try:\n",
        "            experience = wait.until(EC.presence_of_element_located((By.XPATH, experience_xpath))).text\n",
        "            print(experience)\n",
        "        except:\n",
        "            experience = \"NULL\"\n",
        "        try:\n",
        "            salary = wait.until(EC.presence_of_element_located((By.XPATH, salary_xpath))).text\n",
        "            print(salary)\n",
        "        except:\n",
        "            salary = \"Not Disclosed\"\n",
        "        new_index += 1\n",
        "        i += 1\n",
        "        print(\"--------------------------- \"+str(i)+\" ----------------------------------\")\n",
        "        # Writing all the Scrapped data into CSV file.\n",
        "        csv_writer.writerow([heading, subheading, link, experience, salary])\n",
        "        if i >= count:\n",
        "            break\n",
        "    if i >= count:\n",
        "        break\n",
        "    element = driver.find_element(By.XPATH, '//*[text() = \"Next\"]').location_once_scrolled_into_view\n",
        "    driver.execute_script(\"window.scrollBy(0,-120)\")\n",
        "    wait.until(EC.element_to_be_clickable((By.XPATH, '//*[text() = \"Next\"]'))).click()\n",
        "    new_index = 1\n",
        "csv_file.close()"
      ],
      "metadata": {
        "id": "n5SVtCbXL5E-"
      },
      "execution_count": 16,
      "outputs": []
    }
  ]
}